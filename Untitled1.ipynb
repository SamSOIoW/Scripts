{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30934c95-e56f-4bce-b86e-1e46835bb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Build, 'Build', 5389077834083678306), (your, 'your', 1572612192562026184), (data, 'data', 6645506661261177361), (science, 'science', 15907409978124634296), (skills, 'skills', 12917368283846924199), (to, 'to', 3791531372978436496), (launch, 'launch', 1882817931903534611), (an, 'an', 15099054000809333061), (in, 'in', 3002984154512732771), (-, '-', 9153284864653046197), (demand, 'demand', 14960746577302607048), (,, ',', 2593208677638477497), (valuable, 'valuable', 8193598804577691257), (career, 'career', 16267834800013645067), (in, 'in', 3002984154512732771), (six, 'six', 14386902641064052387), (months, 'months', 11199438407386692767), (., '.', 12646065887601541794)]\n",
      "['Build', 'your', 'data', 'science', 'skills', 'to', 'launch', 'an', 'in', 'demand', 'valuable', 'career', 'in', 'six', 'months']\n",
      "your\n",
      "to\n",
      "an\n",
      "in\n",
      "in\n",
      "six\n",
      "['sing', 'singing', 'sing']\n",
      "[(Priyanka Chopra Jonas, 'ORG', 383), (Indian, 'NORP', 381), (2000, 'DATE', 391), (One, 'CARDINAL', 397), (India, 'GPE', 384), (Chopra, 'ORG', 383), (five, 'CARDINAL', 397), (2016, 'DATE', 391), (India, 'GPE', 384), (the Padma Shri, 'PRODUCT', 386), (Time, 'ORG', 383), (one, 'CARDINAL', 397), (100, 'CARDINAL', 397)]\n",
      "FAC:Buildings, airports, highways, bridges, etc.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- spaCy -------------------------------------\n",
    "# spaCy is a Python natural language processing library specifically designed with\n",
    "# the goal of being a useful library for implementing production-ready systems.\n",
    "# It is particularly fast and intuitive, making it a top contender for NLP tasks.\n",
    "\n",
    "# ------------------------- IMPORTANT -------------------------------------\n",
    "#\tPlease make sure you have read and understand the instructions for this task.\n",
    "#\tWe will be working with spaCy which is an EXTERNAL Python module. SpaCy, as\n",
    "#\twell as its English language model, should have been installed at the start of \n",
    "#   your bootcamp. If you have any problems, contact a mentor for support. \n",
    "#   SpaCy MUST BE INSTALLED BEFORE YOU CAN COMPLETE THIS TASK. \n",
    "\n",
    "# ************************** INSTALLATION **************************************\n",
    "# Below are the commands that are needed to install spaCy, FYI. You should not\n",
    "# need to do this as it should have been done for you by a script HyperionDev\n",
    "# runs at the start of your bootcamp, but has been retained here FOR YOUR INTEREST.\n",
    " \n",
    "# Type the following commands in command line\n",
    "# pip3 install spacy\n",
    "# python3 -m spacy download en_core_web_sm  \n",
    "# ----------------OR----------------------\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "# ======= Working with the spaCy ===== #\n",
    "\n",
    "import spacy #This statement should work if you have spaCy installed \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sample = u\"Build your data science skills to launch an in-demand, valuable career in six months.\"\n",
    "\n",
    "doc = nlp(sample)\n",
    "\n",
    "# Tokenisation------------------------------------------------------------------\n",
    "\n",
    "# Tokenisation is a foundational step in many NLP tasks. Tokenising text is the\n",
    "# process of splitting a piece of text into words, symbols, punctuation, spaces,\n",
    "# and other elements, thereby creating “tokens”. A naive way to do this is to\n",
    "# simply split the string on white space:\n",
    "doc.text.split()\n",
    "\n",
    "'''Output: ['Build', 'your', 'data', 'science', 'skills', 'to', 'launch', 'an', 'in-demand,'\n",
    ", 'valuable', 'career', 'in', 'six', 'months.']'''\n",
    "\n",
    "# On the surface, this looks fine. But, note that it disregards the punctuation.\n",
    "# Put differently, it is naive - it fails to recognise elements of the text that help\n",
    "# us (and a machine) to understand its structure and meaning. Let’s see how spaCy handles this:\n",
    "[token.orth_ for token in doc]\n",
    "\n",
    "''' Output: \n",
    "['Build', 'your', 'data', 'science', 'skills', 'to', 'launch', 'an', 'in', '-',\n",
    "'demand', ',', 'valuable', 'career', 'in', 'six', 'months', '.'] '''\n",
    "\n",
    "# Here we access the each token’s .orth_ method, which returns a string representation\n",
    "# of the token rather than a spaCy token object; this might not always be desirable,\n",
    "# but worth noting. SpaCy recognises punctuation and is able to split these punctuation\n",
    "# tokens from word tokens. Many of spaCy’s token methods offer both string and integer\n",
    "# representations of processed text – methods with an underscore suffix return strings,\n",
    "# methods without an underscore suffix return integers. For example:\n",
    "print([(token, token.orth_, token.orth) for token in doc])\n",
    "\n",
    "'''\n",
    "Output: \n",
    "[(Build, 'Build', 5389077834083678306),\n",
    "(your, 'your', 1572612192562026184),\n",
    "(data, 'data', 6645506661261177361), ]\n",
    "...\n",
    "'''\n",
    "# If you want to avoid returning tokens that are punctuation or white space, spaCy\n",
    "# provides convenient methods for this\n",
    "\n",
    "print([token.orth_ for token in doc if not token.is_punct | token.is_space])\n",
    "\n",
    "'''Output:\n",
    "['Build', 'your', 'data', 'science', 'skills', 'to', 'launch', 'an', 'in', 'demand',\n",
    " 'valuable', 'career', 'in', 'six', 'months']\n",
    "'''\n",
    "\n",
    "# Let's identify stop words. We imported the word list above, so it's just a\n",
    "# matter of iterating through the tokens stored in the doc object and performing\n",
    "# a comparison:\n",
    "\n",
    "for word in doc:\n",
    "    if word.is_stop == True:\n",
    "        print(word)\n",
    "'''\n",
    "Output:\n",
    "your\n",
    "to\n",
    "an\n",
    "in\n",
    "in\n",
    "six   '''\n",
    "\n",
    "\n",
    "# Lemmatisation -------------------------------------------------------------\n",
    "\n",
    "# A related task to tokenisation is lemmatisation. Lemmatisation is the process\n",
    "# of reducing a word to its base form, its 'mother word', if you like. Different\n",
    "# uses of a word often have the same root meaning. For example, practise, practised,\n",
    "# and practising all essentially refer to the same thing. It is often desirable\n",
    "# to standardise words with similar meaning to their base form. With spaCy we can\n",
    "# access each word’s base form with a token’s .lemma_ method:\n",
    "\n",
    "sing = \"sang singing sing\"\n",
    "nlp_practice = nlp(sing)\n",
    "print([word.lemma_ for word in nlp_practice])\n",
    "\n",
    "'''Output: ['sing', 'singe', 'sing'] '''\n",
    "\n",
    "# Why is this useful? An immediate use case is in machine learning, specifically\n",
    "# text classification. Lemmatising the text prior to, for example, creating a\n",
    "# “bag-of-words” avoids word duplication and, therefore, allows for the model to\n",
    "# build a clearer picture of patterns of word usage across multiple documents.\n",
    "\n",
    "\n",
    "# Named entity recognition\n",
    "\n",
    "# Named entity recognition is the process of classifying named entities found in a\n",
    "# text into pre-defined categories, such as persons, places, organisations, dates,\n",
    "# etc. spaCy uses a statistical model to classify a broad range of entities,\n",
    "# including persons, events, works-of-art, and nationalities / religions (see the\n",
    "# documentation for more information https://spacy.io/usage/linguistic-features#named-entities).\n",
    "\n",
    "# For example, let’s take the first two sentences from Priyanka Chopra's wikipedia\n",
    "# entry. We will parse this text, then access the identified entities using the\n",
    "# doc object’s .ents method. With this method called on the doc we can access\n",
    "# additional token methods, specifically .label_ and .label:\n",
    "\n",
    "wiki_priyanka = \"\"\"known by her married name Priyanka Chopra Jonas, is an Indian actress,\n",
    "singer, film producer, philanthropist, and the winner of the Miss World 2000 pageant.\n",
    "One of India's highest-paid and most popular celebrities, Chopra has received numerous\n",
    "awards, including a National Film Award and five Filmfare Awards. In 2016, the Government\n",
    "of India honoured her with the Padma Shri, and Time named her one of the 100 most influential people in the world.\"\"\"\n",
    "\n",
    "# Get labels and entities and print them\n",
    "nlp_priyanka = nlp(wiki_priyanka)\n",
    "print([(i, i.label_, i.label) for i in nlp_priyanka.ents])\n",
    "\n",
    "'''Output:\n",
    " [(Priyanka Chopra Jonas, 'PERSON', 378), (Indian, 'NORP', 379), (\n",
    ", 'GPE', 382), (2000, 'CARDINAL', 394), (\n",
    ", 'GPE', 382), (One, 'CARDINAL', 394), (India, 'GPE', 382), (Chopra, 'ORG', 381), (\n",
    ", 'GPE', 382), (a National Film Award, 'EVENT', 385), (five, 'CARDINAL', 394),\n",
    "(Filmfare Awards, 'FAC', 9191306739292312949), (2016, 'DATE', 388), (the Government\n",
    ", 'ORG', 381), (India, 'GPE', 382), (Padma Shri, 'PERSON', 378), (Time, 'ORG', 381),\n",
    "(one, 'CARDINAL', 394), (100, 'CARDINAL', 394)]'''\n",
    "\n",
    "# Get an explanation of an entity and print it\n",
    "entity_fac = spacy.explain(\"FAC\")\n",
    "print(f\"FAC:{entity_fac}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfee16-c000-492c-b959-2237b5e404e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
